{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d3c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from   scipy.stats import gamma as gamma_dist\n",
    "import matplotlib.pyplot as plt\n",
    "import bayesflow as bf\n",
    "import tensorflow as tf\n",
    "\n",
    "# CONSTANTS\n",
    "ALPHA      = 9                # Gamma shape (From Original Artical)\n",
    "ETA_CONST  = 1e-3             # saliency noise – From Original Artical)\n",
    "PATH       = \"fixseqin_PB2expVP10.dat\"   #data file\n",
    "\n",
    "# 0)  LOAD FIXATIONS   \n",
    "def load_fixations(path=PATH):\n",
    "    cols = ['sentID','fixNum','onset','dur_ms','wordID',\n",
    "            'x','y','line','flag1','flag2']\n",
    "    return (\n",
    "        pd.read_csv(path, sep=r'\\s+', header=None, names=cols,\n",
    "                    usecols=['sentID','wordID','dur_ms'])\n",
    "          .query(\"dur_ms >= 50\")          # remove microsaccades, For \"cognative Fixations It is important\"\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "df           = load_fixations()\n",
    "sent_lens    = df.groupby(\"sentID\")[\"wordID\"].max().to_dict()   # {id: N_words}\n",
    "all_sent_ids = np.array(list(sent_lens), dtype=int)\n",
    "GLOBAL_PAD   = max(sent_lens.values())\n",
    "\n",
    "# 1)  SIMULATOR  (9 raw channels, uses fixed η)\n",
    "def gamma_timer(mu_T):\n",
    "    \"\"\"Draw a single fixation duration ∼ Gamma(α, scale=μ_T/α).\"\"\"\n",
    "    return np.random.gamma(ALPHA, mu_T / ALPHA)\n",
    "\n",
    "def swift_baseline(nu, r, mu_T, *, sent_ids, pad_len):  \n",
    "    \"\"\"\n",
    "    Produce one batch of padded scan-paths with **fixed** η.\n",
    "    Shapes of all channels: (B, T, 1)  — no extra dummy axes.\n",
    "    \"\"\"\n",
    "    sent_ids = np.atleast_1d(sent_ids)          # (B,) even if B=1\n",
    "    pad_len  = np.atleast_1d(pad_len)           # same\n",
    "    B        = sent_ids.shape[0]\n",
    "    T        = int(pad_len[0])\n",
    "    zeros = lambda: np.zeros((B, T, 1), np.float32)\n",
    "\n",
    "    fd, fw = zeros(), zeros()      # fix_dur, fix_word\n",
    "    sl, sd = zeros(), zeros()      # sacc_len, sacc_dir\n",
    "    le     = zeros()               # landing_error\n",
    "    skp, rf= zeros(), zeros()      # skip_flag, refix_flag\n",
    "\n",
    "    for b, sid in enumerate(sent_ids):\n",
    "        N = sent_lens[int(sid)]\n",
    "        for w in range(N):\n",
    "            fd[b, w, 0] = gamma_timer(mu_T[b])\n",
    "            fw[b, w, 0] = w + 1\n",
    "            L           = np.random.poisson(r[b])\n",
    "            sl[b, w, 0] = L\n",
    "            sd[b, w, 0] = 1.0                      # always forward\n",
    "            le[b, w, 0] = np.random.normal(0., 5. * ETA_CONST)\n",
    "\n",
    "            p = 1 / (1 + np.exp(-20 * (nu[b] - 0.5)))\n",
    "            skipped = np.random.rand() < p\n",
    "            skp[b, w, 0] = skipped\n",
    "            if not skipped:\n",
    "                rf[b, w, 0] = np.random.rand() < p\n",
    "\n",
    "    # sentence-level summary channels, copied along T\n",
    "    skip_rate  = np.repeat(skp.mean(axis=1, keepdims=True), T, axis=1)\n",
    "    refix_rate = np.repeat(rf .mean(axis=1, keepdims=True), T, axis=1)\n",
    "\n",
    "    return dict(fix_dur=fd, fix_word=fw, sacc_len=sl, sacc_dir=sd,\n",
    "                landing_error=le, skip_flag=skp, refix_flag=rf,\n",
    "                skip_rate=skip_rate, refix_rate=refix_rate)\n",
    "\n",
    "# 2)  PRIOR, META,  SIMULATOR WRAPPER\n",
    "def sample_prior(batch_shape=None, **_):\n",
    "    B = int(np.prod(batch_shape)) if batch_shape else 1\n",
    "    return dict(\n",
    "        nu   = np.random.uniform(0, 1 , B),\n",
    "        r    = np.random.uniform(0 , 12 , B),\n",
    "        mu_T = np.random.uniform(100, 400, B)\n",
    "    )\n",
    "\n",
    "def batch_meta(batch_shape):\n",
    "    B    = int(batch_shape[0])\n",
    "    sids = np.random.choice(all_sent_ids, size=B)\n",
    "    return dict(sent_ids=sids.astype(np.int32),\n",
    "                pad_len=np.full(B, GLOBAL_PAD, np.int32))\n",
    "\n",
    "def simulator_fn(nu, r, mu_T, *, sent_ids, pad_len):\n",
    "    \"\"\"Wrapper that feeds fixed η to the baseline generator.\"\"\"\n",
    "    return swift_baseline(nu, r, mu_T,\n",
    "                          sent_ids = sent_ids,\n",
    "                          pad_len  = pad_len)\n",
    "\n",
    "simulator = bf.make_simulator(\n",
    "    [sample_prior, simulator_fn],  # building blocks\n",
    "    {},                             # no object kwargs\n",
    "    batch_meta\n",
    ")\n",
    "\n",
    "# 3)  NETWORKS  +  ADAPTER\n",
    "inf_net = bf.networks.FlowMatching()\n",
    "sum_net = bf.networks.SetTransformer(\n",
    "    embed_dims =(64,), num_heads =(4,), mlp_depths=(2,), mlp_widths=(128,)\n",
    ")\n",
    "\n",
    "adapter = (\n",
    "    bf.Adapter()\n",
    "      .concatenate([\"nu\", \"r\", \"mu_T\"], into=\"inference_variables\")\n",
    "\n",
    "      # (B,1,T,1) → (B,T,1)\n",
    "      .squeeze(\n",
    "          [\"fix_dur\",\"fix_word\",\"sacc_len\",\"sacc_dir\",\"landing_error\",\n",
    "           \"skip_flag\",\"refix_flag\",\"skip_rate\",\"refix_rate\"],\n",
    "          axis=1)\n",
    "\n",
    "      #  9 channel → summary_variables  (B,T,9)\n",
    "      .concatenate(\n",
    "          [\"fix_dur\",\"fix_word\",\"sacc_len\",\"sacc_dir\",\"landing_error\",\n",
    "           \"skip_flag\",\"refix_flag\",\"skip_rate\",\"refix_rate\"],\n",
    "          into=\"summary_variables\")\n",
    ")\n",
    "\n",
    "\n",
    "# 4)  WORKFLOW\n",
    "workflow = bf.BasicWorkflow(\n",
    "    simulator        = simulator,\n",
    "    adapter          = adapter,\n",
    "    inference_network= inf_net,\n",
    "    summary_network  = sum_net,\n",
    "    inference_variables = [\"nu\", \"r\", \"mu_T\"],\n",
    "\n",
    ")\n",
    "print(\"Workflow initialised, ready to train.\")\n",
    "\n",
    "# For Debug check:   \n",
    "raw_test = simulator.sample((4,))        \n",
    "print(\"\\nMini-batch channel shapes:\")\n",
    "for k, v in raw_test.items():\n",
    "    print(f\"{k:12s} {v.shape}\")\n",
    "\n",
    "# 5)  Training Part\n",
    "history = workflow.fit_online(\n",
    "    epochs                = 50,\n",
    "    num_batches_per_epoch = 200,\n",
    "    batch_size            = 64,\n",
    "    validation_data       = 200,     # → every epoch: 200 fresh sims\n",
    "    loss                  = \"mmd\",\n",
    "    optimizer             = tf.keras.optimizers.Adam(1e-3),\n",
    "    verbose               = 2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=6,          \n",
    "            min_delta=0.0,      \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ],\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nFirst 5 losses:\", history.history[\"loss\"][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67403eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayesflow.diagnostics as bfd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "S = 200                     \n",
    "raw_test = workflow.simulate(batch_shape=(S,))   # simulator → raw dict\n",
    "test_data = workflow.adapter(raw_test)           \n",
    "\n",
    "\n",
    "M = 1000\n",
    "posterior = workflow.sample(\n",
    "    num_samples = M,\n",
    "    conditions  = test_data      \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "figs = workflow.plot_default_diagnostics(\n",
    "    test_data   = raw_test,   \n",
    "    num_samples = M\n",
    ")\n",
    "\n",
    "for name, fig in figs.items():  \n",
    "    print(name)\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad8b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_scans_to_raw(scans, pad_len):\n",
    "    \"\"\"\n",
    "    Nine channels, each shape (B, 1, pad_len, 1)  ← cümle ekseni = 1\n",
    "    \"\"\"\n",
    "    B, T = len(scans), pad_len\n",
    "    zeros = lambda: np.zeros((B, 1, T, 1), np.float32)\n",
    "\n",
    "    raw = dict(fix_dur=zeros(),  fix_word=zeros(),\n",
    "               sacc_len=zeros(), sacc_dir=zeros(),\n",
    "               landing_error=zeros(),\n",
    "               skip_flag=zeros(), refix_flag=zeros(),\n",
    "               skip_rate=zeros(),  refix_rate=zeros())\n",
    "\n",
    "    for b, sc in enumerate(scans):\n",
    "        n = len(sc[\"fix_dur\"])\n",
    "        raw[\"fix_dur\"][b, 0, :n, 0]   = sc[\"fix_dur\"]\n",
    "        raw[\"fix_word\"][b,0, :n, 0]   = sc[\"fix_word\"]\n",
    "        raw[\"sacc_len\"][b,0, :n, 0]   = sc[\"sacc_len\"]\n",
    "        raw[\"sacc_dir\"][b,0, :n, 0]   = np.sign(sc[\"sacc_len\"])\n",
    "        raw[\"skip_flag\"][b,0,:n,0]    = sc[\"skip_flag\"]\n",
    "        raw[\"refix_flag\"][b,0,:n,0]   = sc[\"refix_flag\"]\n",
    "\n",
    "        raw[\"skip_rate\"][b,0,:,0]   = sc[\"skip_flag\"].mean()\n",
    "        raw[\"refix_rate\"][b,0,:,0]  = sc[\"refix_flag\"].mean()\n",
    "    return raw\n",
    "\n",
    "def build_scanpaths(df):\n",
    "    scans=[]\n",
    "    for sid,g in df.groupby('sentID',sort=True):\n",
    "        g = g.sort_values('fixNum') if 'fixNum' in g else g.sort_index()\n",
    "        w_ids = g.wordID.to_numpy()\n",
    "        durs  = g.dur_ms.to_numpy(float)\n",
    "        sacc  = np.diff(np.r_[w_ids[0], w_ids])\n",
    "        skip  = (np.abs(sacc) > 1).astype(int)\n",
    "        refix = np.r_[0,(w_ids[1:]==w_ids[:-1]).astype(int)]\n",
    "        scans.append(dict(\n",
    "            fix_word   = w_ids.astype(int),\n",
    "            fix_dur    = durs,\n",
    "            sacc_len   = sacc.astype(int),\n",
    "            skip_flag  = skip,\n",
    "            refix_flag = refix,\n",
    "        ))\n",
    "    return scans\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- REAL PARTICIPANT INFERENCE ------------------\n",
    "df_real   = load_fixations(PATH)\n",
    "DATA_REAL = build_scanpaths(df_real)\n",
    "\n",
    "real_raw  = pack_scans_to_raw(DATA_REAL, GLOBAL_PAD)\n",
    "# add dummy θ keys so that adapter finds them\n",
    "real_raw[\"nu\"]   = np.array([[0.]], np.float32)\n",
    "real_raw[\"r\"]    = np.array([[0.]], np.float32)\n",
    "real_raw[\"mu_T\"] = np.array([[0.]], np.float32)\n",
    "\n",
    "cond      = workflow.adapter(real_raw, stage=\"predict\")\n",
    "\n",
    "posterior = workflow.sample(num_samples=1000, conditions=cond)\n",
    "\n",
    "print(\"\\nPosterior means (reader-specific):\")\n",
    "for p, v in posterior.items():\n",
    "    print(f\"{p:4s}: μ={v.mean():7.3f}  σ={v.std():6.3f}\")\n",
    "\n",
    "plt.figure(figsize=(8,2.5))\n",
    "for i,(p,v) in enumerate(posterior.items(),1):\n",
    "    plt.subplot(1,len(posterior),i)\n",
    "    plt.hist(v.flatten(), 40, density=True, alpha=.6)\n",
    "    plt.axvline(v.mean(), c='k', ls='--'); plt.title(p)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose the first subject (index 0)\n",
    "subject_idx = 0\n",
    "\n",
    "# Extract and reshape to (num_samples, num_variables)\n",
    "samples = np.column_stack([\n",
    "    posterior[\"nu\"][subject_idx].flatten(),\n",
    "    posterior[\"r\"][subject_idx].flatten(),\n",
    "    posterior[\"mu_T\"][subject_idx].flatten()\n",
    "])\n",
    "\n",
    "# Convert to dict as expected by BayesFlow\n",
    "posterior_dict_single = {\n",
    "    \"nu\": samples[:, 0][:, None],\n",
    "    \"r\": samples[:, 1][:, None],\n",
    "    \"mu_T\": samples[:, 2][:, None]\n",
    "}\n",
    "\n",
    "grid = bf.diagnostics.plots.pairs_posterior(\n",
    "    estimates=posterior_dict_single,\n",
    "    variable_keys=[\"nu\", \"r\", \"mu_T\"],\n",
    "    height=2.3,\n",
    "    alpha=0.35\n",
    ")\n",
    "\n",
    "grid.map_diag(sns.histplot, bins=40, color=\"#1f77b4\", edgecolor=\"w\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784715d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nu 0.8789034 0.16725872 0.36488742 1.3749603\n",
      "r 1.5622747 0.814267 -1.3081803 6.244174\n",
      "mu_T 123.00348 19.42611 36.833023 218.0245\n"
     ]
    }
   ],
   "source": [
    "# For Debug Check Posterior Values.\n",
    "\n",
    "\n",
    "for k, v in posterior_dict.items():\n",
    "    print(k, v.mean(), v.std(), v.min(), v.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ffebac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 1.26.4\n",
      "pandas: 2.2.3\n",
      "scipy: 1.15.2\n",
      "matplotlib: 3.10.1\n",
      "seaborn: 0.13.2\n",
      "tensorflow: 2.19.0\n",
      "bayesflow: 2.0.4\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import scipy\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import tensorflow\n",
    "import bayesflow\n",
    "\n",
    "print(\"numpy:\", numpy.__version__)\n",
    "print(\"pandas:\", pandas.__version__)\n",
    "print(\"scipy:\", scipy.__version__)\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n",
    "print(\"seaborn:\", seaborn.__version__)\n",
    "print(\"tensorflow:\", tensorflow.__version__)\n",
    "print(\"bayesflow:\", bayesflow.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (advanced-python)",
   "language": "python",
   "name": "advanced-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
